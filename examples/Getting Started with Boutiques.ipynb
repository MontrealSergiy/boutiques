{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Boutiques\n",
    "\n",
    "As you've seen from our documentation, Boutiques is a flexible way to represent command line executables and distribute them across compute ecosystems consistently. A Boutiques tool descriptor is a JSON file that fully describes the input and output parameters and files for a given command line call (or calls, as you can include pipes(`|`) and ampersands (`&`)). There are several ways Boutiques helps you build a tool descriptor for your tool:\n",
    "\n",
    "- The [boutiques command-line utility](https://github.com/boutiques/boutiques/) contains a validator, simulator, and other tools which can help you either find an existing descriptor you wish to model yours after, or build and test your own.\n",
    "- The [examples](https://github.com/aces/cbrain-plugins-neuro/tree/master/cbrain_task_descriptors) provide useful references for development.\n",
    "\n",
    "To help you aid in this process, we will walk through the process of making an tool descriptor for [FSL's BET](http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/BET) (finished product found [here](https://github.com/aces/cbrain-plugins-neuro/blob/master/cbrain_task_descriptors/fsl_bet.json))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Describing the command line\n",
    "\n",
    "The first step in creating an tool descriptor for your command line call is creating a fully descriptive list of your command line options. If your tool was written in Python and you use the `argparse` library, then this is already done for you in large part. For many tools (bash, Python, or otherwise) this can be obtained by typing executing it with the `-h` flag. In the case of FSL's BET, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:    bet <input> <output> [options]\n",
      "\n",
      "Main bet2 options:\n",
      "  -o          generate brain surface outline overlaid onto original image\n",
      "  -m          generate binary brain mask\n",
      "  -s          generate approximate skull image\n",
      "  -n          don't generate segmented brain image output\n",
      "  -f <f>      fractional intensity threshold (0->1); default=0.5; smaller values give larger brain outline estimates\n",
      "  -g <g>      vertical gradient in fractional intensity threshold (-1->1); default=0; positive values give larger brain outline at bottom, smaller at top\n",
      "  -r <r>      head radius (mm not voxels); initial surface sphere is set to half of this\n",
      "  -c <x y z>  centre-of-gravity (voxels not mm) of initial mesh surface.\n",
      "  -t          apply thresholding to segmented brain image and mask\n",
      "  -e          generates brain surface as mesh in .vtk format\n",
      "\n",
      "Variations on default bet2 functionality (mutually exclusive options):\n",
      "  (default)   just run bet2\n",
      "  -R          robust brain centre estimation (iterates BET several times)\n",
      "  -S          eye & optic nerve cleanup (can be useful in SIENA)\n",
      "  -B          bias field & neck cleanup (can be useful in SIENA)\n",
      "  -Z          improve BET if FOV is very small in Z (by temporarily padding end slices)\n",
      "  -F          apply to 4D FMRI data (uses -f 0.3 and dilates brain mask slightly)\n",
      "  -A          run bet2 and then betsurf to get additional skull and scalp surfaces (includes registrations)\n",
      "  -A2 <T2>    as with -A, when also feeding in non-brain-extracted T2 (includes registrations)\n",
      "\n",
      "Miscellaneous options:\n",
      "  -v          verbose (switch on diagnostic messages)\n",
      "  -h          display this help, then exits\n",
      "  -d          debug (don't delete temporary intermediate images)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bet -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all of these flags, we see a list of options which can be summarized by:\n",
    "```\n",
    "bet [INPUT_FILE] [MASK] [FRACTIONAL_INTENSITY] [VERTICAL_GRADIENT] [CENTER_OF_GRAVITY] [OVERLAY_FLAG] [BINARY_MASK_FLAG] [APPROX_SKULL_FLAG] [NO_SEG_OUTPUT_FLAG] [VTK_VIEW_FLAG] [HEAD_RADIUS] [THRESHOLDING_FLAG] [ROBUST_ITERS_FLAG] [RES_OPTIC_CLEANUP_FLAG] [REDUCE_BIAS_FLAG] [SLICE_PADDING_FLAG] [MASK_WHOLE_SET_FLAG] [ADD_SURFACES_FLAG] [ADD_SURFACES_T2] [VERBOSE_FLAG] [DEBUG_FLAG]\n",
    "```\n",
    "\n",
    "Now that we have summarized all command line options for our tool - some of which describe inputs and others, outputs - we can begin to craft our JSON Boutiques tool descriptor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding Boutiques + JSON\n",
    "\n",
    "For those unfamiliar with JSON, we recommend following this [3 minute JSON tutorial](http://www.secretgeek.net/json_3mins) to get you up to speed. In short, a JSON file is a dictionary object which contains *keys* and associated *values*. A *key* informs us what is being described, and a *value* is the description (which, importantly, can be arbitrarily typed). The Boutiques tool descriptor is a JSON file which requires the following keys, or, properties:\n",
    "- `name`\n",
    "- `description`\n",
    "- `schema-version`\n",
    "- `command-line`\n",
    "- `inputs`\n",
    "- `output-files`\n",
    "\n",
    "Some additional, optional, properties that a Boutiques fill will recognize are:\n",
    "- `groups`\n",
    "- `tool-version`\n",
    "- `suggested-resources`\n",
    "- `container-image`:\n",
    "  - `type`\n",
    "  - `image`\n",
    "  - `index`\n",
    "\n",
    "In the case of BET, we will of course populate the required elements, but will also include `tool-version` and `groups`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Populating the tool descriptor\n",
    "\n",
    "We will break-up populating the tool descriptor into two sections: adding meta-parameters (such as `name`, `description`, `schema-version`, `command-line`, `tool-version`, and `docker-image`, `docker-index` if we were to include them) and i/o-parameters (such as `inputs`, `output-files`, and `groups`).\n",
    "\n",
    "Currently, before adding any details, our tool descriptor should looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"name\" : TODO,\n",
    "    \"tool-version\": TODO,\n",
    "    \"description\": TODO,\n",
    "    \"command-line\": TODO,\n",
    "    \"scheme-version\": TODO,\n",
    "    \"inputs\": TODO,\n",
    "    \"output-files\": TODO,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3.1: Adding meta-parameters\n",
    "\n",
    "Many of the meta-parameters will be obvious to you if you're familiar with the tool, or extractable from the message received earlier when you passed the `-h` flag into your program. We can update our JSON to be the following:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"name\" : \"fsl_bet\",\n",
    "\t\"tool-version\" : \"1.0.0\",\n",
    "\t\"description\" : \"Automated brain extraction tool for FSL\",\n",
    "\t\"command-line\" : \"bet [INPUT_FILE] [MASK] [FRACTIONAL_INTENSITY] [VERTICAL_GRADIENT] [CENTER_OF_GRAVITY] [OVERLAY_FLAG] [BINARY_MASK_FLAG] [APPROX_SKULL_FLAG] [NO_SEG_OUTPUT_FLAG] [VTK_VIEW_FLAG] [HEAD_RADIUS] [THRESHOLDING_FLAG] [ROBUST_ITERS_FLAG] [RES_OPTIC_CLEANUP_FLAG] [REDUCE_BIAS_FLAG] [SLICE_PADDING_FLAG] [MASK_WHOLE_SET_FLAG] [ADD_SURFACES_FLAG] [ADD_SURFACES_T2] [VERBOSE_FLAG] [DEBUG_FLAG]\",\n",
    "\t\"schema-version\" : \"0.4\",\n",
    "    \"inputs\": TODO,\n",
    "    \"output-files\": TODO,\n",
    "    \"groups\": TODO\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Adding i/o parameters\n",
    "\n",
    "Inputs and outputs of many applications are complicated - outputs can be dependent upon input flags, flags can be mutually exclusive or require at least one option, etc. The way Boutiques handles this is with a detailed schema which consists of options for inputs and outputs, as well as optionally specifying groups of inputs which may add additional layers of input complexity.\n",
    "\n",
    "As you have surely noted, tools do only contain a single \"name\" or \"version\" being used, but may have many input and output parameters. This means that inputs, outputs, and groups, will be described as a list. Each element of these lists will be a dictionary following the input, output, or group schema, respectively. This means that our JSON actually looks more like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"name\" : \"fsl_bet\",\n",
    "\t\"tool-version\" : \"1.0.0\",\n",
    "\t\"description\" : \"Automated brain extraction tool for FSL\",\n",
    "\t\"command-line\" : \"bet [INPUT_FILE] [MASK] [FRACTIONAL_INTENSITY] [VERTICAL_GRADIENT] [CENTER_OF_GRAVITY] [OVERLAY_FLAG] [BINARY_MASK_FLAG] [APPROX_SKULL_FLAG] [NO_SEG_OUTPUT_FLAG] [VTK_VIEW_FLAG] [HEAD_RADIUS] [THRESHOLDING_FLAG] [ROBUST_ITERS_FLAG] [RES_OPTIC_CLEANUP_FLAG] [REDUCE_BIAS_FLAG] [SLICE_PADDING_FLAG] [MASK_WHOLE_SET_FLAG] [ADD_SURFACES_FLAG] [ADD_SURFACES_T2] [VERBOSE_FLAG] [DEBUG_FLAG]\",\n",
    "\t\"schema-version\" : \"0.4\",\n",
    "    \"inputs\": [\n",
    "        {TODO},\n",
    "        {TODO},\n",
    "        ...\n",
    "    ],\n",
    "    \"output-files\": [\n",
    "        {TODO},\n",
    "        {TODO},\n",
    "        ...\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "As the file is beginning to grow considerably in number of lines, we will no longer show you the full JSON at each step but will simply show you the dictionaries responsible for output, input, and group entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.1: Specifying inputs\n",
    "\n",
    "The input schema contains several options, many of which can be ignored in this first example with the exception of `id`, `name`, and `type`. For BET, there are several input values we can choose to demonstrate this for you. We have chosen three with considerably different functionality and therefore schemas. In particular:\n",
    "- `[INPUT_FILE]`\n",
    "- `[FRACTIONAL_INTENSITY]`\n",
    "- `[CENTER_OF_GRAVITY]`\n",
    "\n",
    "**`[INPUT_FILE]`**   The simplest of these in the `[INPUT_FILE]` which is a required parameter that simply expects a qualified path to a file. The dictionary entry is:\n",
    "```\n",
    "{\n",
    "    \"id\" : \"infile\",\n",
    "    \"name\" : \"Input file\",\n",
    "    \"type\" : \"File\",\n",
    "    \"description\" : \"Input image (e.g. img.nii.gz)\",\n",
    "    \"optional\": false,\n",
    "    \"value-key\" : \"[INPUT_FILE]\"\n",
    "}\n",
    "```\n",
    "\n",
    "**`[FRACTIONAL_INTENSITY]`**   This parameter documents an optional flag that can be passed to the executable. Along with the flag, when it is passed, is a floating point value that can range from 0 to 1. We are able to validate at the level of Boutiques whether or not a valid input is passed, so that jobs are not submitted to the execution engine which will error, but they get flagged upon validation of inputs. This dictionary is:\n",
    "```\n",
    "{\n",
    "    \"id\" : \"fractional_intensity\",\n",
    "    \"name\" : \"Fractional intensity threshold\",\n",
    "    \"type\" : \"Number\",\n",
    "    \"description\" : \"Fractional intensity threshold (0->1); default=0.5; smaller values give larger brain outline estimates\",\n",
    "    \"command-line-flag\": \"-f\",\n",
    "    \"optional\": true,\n",
    "    \"value-key\" : \"[FRACTIONAL_INTENSITY]\",\n",
    "    \"integer\" : false,\n",
    "    \"minimum\" : 0,\n",
    "    \"maximum\" : 1\n",
    "}\n",
    "```\n",
    "\n",
    "**`[CENTER_OF_GRAVITY]`**   The center of gravity value expects a triple (i.e. [X, Y, Z] position) if the flag is specified. Here we are able to set the condition that the length of the list received after this flag is 3, by specifying that the input is a list that has both a minimum and maximum length.\n",
    "```\n",
    "{\n",
    "    \"id\" : \"center_of_gravity\",\n",
    "    \"name\" : \"Center of gravity vector\",\n",
    "    \"type\" : \"Number\",\n",
    "    \"description\" : \"The xyz coordinates of the center of gravity (voxels, not mm) of initial mesh surface. Must have exactly three numerical entries in the list (3-vector).\",\n",
    "    \"command-line-flag\": \"-c\",\n",
    "    \"optional\": true,\n",
    "    \"value-key\" : \"[CENTER_OF_GRAVITY]\",\n",
    "    \"list\" : true,\n",
    "    \"min-list-entries\" : 3,\n",
    "    \"max-list-entries\" : 3\n",
    "}\n",
    "```\n",
    "\n",
    "For further examples of different types of inputs, feel free to explore [more examples](https://github.com/aces/cbrain-plugins-neuro/tree/master/cbrain_task_descriptors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.2: Specifying outputs\n",
    "\n",
    "The output schema also contains several options, with the only mandatory ones being `id`, `name`, and `path-template`. We again demonstrate an example from BET:\n",
    "- `outfile`\n",
    "\n",
    "**`outfile`**   All of the output parameters in BET are similarly structured, and exploit the same core functionality of basing the output file, described by `path-template`, as a function of an input value on the command line, here given by `[MASK]`. The `optional` flag also describes whether or not a derivative should always be produced, and whether Boutiques should indicate an error if a file isn't found. The output descriptor is thus:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"id\" : \"outfile\",\n",
    "    \"name\" : \"Output mask file\",\n",
    "    \"description\" : \"Main default mask output of BET\",\n",
    "    \"path-template\" : \"[MASK].nii.gz\",\n",
    "    \"optional\" : true\n",
    "}\n",
    "```\n",
    "\n",
    "An extension of the feature of naming outputs based on inputs exists in newer versions of the schema than this example was originally developed, and enable stripping the extension of the input values used, as well. An example of this can be seen [here](https://github.com/neurodata/boutiques-tools/blob/master/cbrain_task_descriptors/ndmg.json#L158)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.3: Specifying groups\n",
    "\n",
    "The group schema enables provides an additional layer of complexity when considering the relationships between inputs. For instance, if multiple inputs within a set are mutually exclusive, they may be grouped and a flag set indicating that only one can be selected. Alternatively, if at least one option within a group must be specified, the user can also set a flag indicating such. The following group from the BET implementation is used to illustrate this:\n",
    "- `variational_params_group`\n",
    "\n",
    "**`variational_params_group`**   Many flags exist in BET, and each of them is represented in the command line we specified earlier. However, as you may have noticed when reading the output of `bet -h`, several of these options are mutually exclusive to one another. In order to again prevent jobs from being submitted to a scheduler and failing there, Boutiques enables grouping of inputs and forcing such mutual exclusivity so that the invalid inputs are flagged in the validation stage. This group dictionary is:\n",
    "```\n",
    "{\n",
    "    \"id\" : \"variational_params_group\",\n",
    "    \"name\" : \"Variations on Default Functionality\",\n",
    "    \"description\" : \"Mutually exclusive options that specify variations on how BET should be run.\",\n",
    "    \"members\" : [\"robust_iters_flag\", \"residual_optic_cleanup_flag\", \"reduce_bias_flag\", \"slice_padding_flag\", \"whole_set_mask_flag\", \"additional_surfaces_flag\", \"additional_surfaces_t2\"],\n",
    "    \"mutually-exclusive\" : true\n",
    "}\n",
    "```\n",
    "\n",
    "Though an example of `one-is-required` input groups is not available in our BET example, you can investigate a validated tool descriptor [here](https://github.com/neurodata/boutiques-tools/blob/master/cbrain_task_descriptors/ndmg.json#L13) to see how it is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: (optional) Extending the tool descriptor\n",
    "\n",
    "Now that the basic implementation of this tool has been done, you can check out the [schema](https://github.com/boutiques/boutiques/blob/master/tools/python/boutiques/schema/descriptor.schema.json) to explore deeper functionality of Boutiques. For example, if you have created a Docker or Singularity container, you can associate an image with your tool descriptor and any compute resource with Docker or Singularity installed will launch the executable through them (an example of using Docker can be found [here](https://github.com/neurodata/boutiques-tools/blob/master/cbrain_task_descriptors/ndmg.json#L6))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validating the tool descriptor\n",
    "\n",
    "Once you've completed your Boutiques tool descriptor, you should run the [validator](https://github.com/boutiques/boutiques#validation) to ensure that you have created it correctly. The `README.md` [here](https://github.com/boutiques/boutiques/) describes how to install and use the validator and remainder of the Boutiques shell (`bosh`) tools on your tool descriptor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using the tool descriptor\n",
    "\n",
    "Once the tool descriptor has been validated, your tool is now ready to be integrated in a platform that supports Boutiques. You can use the `localExec.py` tool described [here](https://github.com/boutiques/boutiques/tree/master/tools) to launch your container locally for preliminary testing. Once you feel comfortable with your tool, you can contact your system administrator and have them integrate it into their compute resources so you can test and use it to process your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
